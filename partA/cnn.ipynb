{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1z1M1UQapMBUIvMLgW2eX_yfUGTZGzyA6",
      "authorship_tag": "ABX9TyPgfS/UW+IrGxt0lzTbVQC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdiY2j/CS6910_Assignment2/blob/main/partA/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx8Dds_hBsU6",
        "outputId": "d43a3b36-4c66-4b36-b11c-507f17e32d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwBBJkbJ29aT",
        "outputId": "2a52eeb8-f056-4eba-8fca-ada4a278998a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.44.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "DTXQgMohM-u5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-PEn-WQ3EFs",
        "outputId": "01737dc8-51f5-4e68-cec7-37684265e350"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afzYGChN3G4m",
        "outputId": "f3bfacd8-78f5-43ff-b883-9774062f582b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ExGO_PmZNeMp",
        "outputId": "16f77df7-5f52-4af6-b38b-c126762ce5b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -a '/content/drive/MyDrive/inaturalist_12K/' '/content/inaturalist/'"
      ],
      "metadata": {
        "id": "IprMHfZamdNV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch_size, data_aug):\n",
        "    # Define the directory containing your dataset\n",
        "    train_dir = '/content/inaturalist/train'\n",
        "    test_dir = '/content/inaturalist/val'\n",
        "\n",
        "    if data_aug :\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else :\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    # Create a PyTorch dataset from the image folder\n",
        "    train_dataset = ImageFolder(train_dir, transform=transform)\n",
        "    test_dataset = ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "    validation_ratio = 0.2\n",
        "    class_labels = [label for _, label in train_dataset]\n",
        "\n",
        "    # Create a dictionary to store indices for each class\n",
        "    class_id = defaultdict(list)\n",
        "    for idx, label in enumerate(class_labels):\n",
        "        class_id[label].append(idx)\n",
        "\n",
        "    # Initialize lists to store training and validation indices\n",
        "    train_indices = []\n",
        "    val_indices = []\n",
        "\n",
        "    # Split each class into training and validation indices\n",
        "    for y, ids in class_id.items():\n",
        "        num_samples = len(ids)\n",
        "        val_samples = int(validation_ratio * num_samples)\n",
        "        np.random.shuffle(ids)  # Shuffle indices for random selection\n",
        "        train_indices.extend(ids[val_samples:])\n",
        "        val_indices.extend(ids[:val_samples])\n",
        "\n",
        "    # Create SubsetRandomSampler for training and validation sets\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    # Create dataloaders for training and validation sets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "Oa_JvoClCACI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeciesCNN(nn.Module):\n",
        "    def __init__(self, num_classes, num_filters=32, filter_size=[3, 3, 3, 3, 3], dense_neurons=512, activation = 'ReLU', batch_norm = False, dropout_val = 0.0, filter_org = 'double'):\n",
        "        super(SpeciesCNN, self).__init__()\n",
        "        act_func = nn.ReLU()\n",
        "        match activation :\n",
        "            case 'ReLU':\n",
        "                act_func = nn.ReLU\n",
        "            case 'GELU':\n",
        "                act_func = nn.GELU\n",
        "            case 'SiLU':\n",
        "                act_func = nn.SiLU\n",
        "            case 'Mish':\n",
        "                act_func = nn.Mish\n",
        "            case 'LeakyReLU':\n",
        "                act_func = nn.LeakyReLU\n",
        "            case 'Sigmoid':\n",
        "                act_func = nn.Sigmoid\n",
        "\n",
        "        match filter_org :\n",
        "            case 'same':\n",
        "                filters = [num_filters] * 5\n",
        "            case 'double':\n",
        "                filters = [num_filters * (2 ** i) for i in range(5)]\n",
        "            case 'half':\n",
        "                filters = [num_filters // (2 ** i) for i in range(5)]\n",
        "\n",
        "\n",
        "\n",
        "        # Convolutional block 1\n",
        "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=filters[0], kernel_size=filter_size[0], padding=1)\n",
        "        self.act_1 = act_func()\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        if batch_norm :\n",
        "            self.batch_1 = nn.BatchNorm2d(filters[0])\n",
        "\n",
        "        # Convolutional block 2\n",
        "        self.conv_2 = nn.Conv2d(in_channels=filters[0], out_channels=filters[1], kernel_size=filter_size[1], padding=1)\n",
        "        self.act_2 = act_func()\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        if batch_norm :\n",
        "            self.batch_2 = nn.BatchNorm2d(filters[1])\n",
        "\n",
        "        # Convolutional block 3\n",
        "        self.conv_3 = nn.Conv2d(in_channels=filters[1], out_channels=filters[2], kernel_size=filter_size[2], padding=1)\n",
        "        self.act_3 = act_func()\n",
        "        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        if batch_norm :\n",
        "            self.batch_3 = nn.BatchNorm2d(filters[2])\n",
        "\n",
        "        # Convolutional block 4\n",
        "        self.conv_4 = nn.Conv2d(in_channels=filters[2], out_channels=filters[3], kernel_size=filter_size[3], padding=1)\n",
        "        self.act_4 = act_func()\n",
        "        self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        if batch_norm :\n",
        "            self.batch_4 = nn.BatchNorm2d(filters[3])\n",
        "\n",
        "        # Convolutional block 5\n",
        "        self.conv_5 = nn.Conv2d(in_channels=filters[3], out_channels=filters[4], kernel_size=filter_size[4], padding=1)\n",
        "        self.act_5 = act_func()\n",
        "        self.pool_5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        if batch_norm :\n",
        "            self.batch_5 = nn.BatchNorm2d(filters[4])\n",
        "\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = nn.LazyLinear(out_features=dense_neurons, bias=True, device=None, dtype=None)\n",
        "        self.fc1_activation = act_func()\n",
        "        if dropout_val > 0.0 :\n",
        "            self.dropout1 = nn.Dropout(dropout_val)\n",
        "        self.fc2 = nn.Linear(dense_neurons, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool_1(self.act_1(self.conv_1((x))))\n",
        "        if hasattr(self, 'batch_1'):\n",
        "            x = self.batch_1(x)\n",
        "\n",
        "        x = self.pool_2(self.act_2(self.conv_2((x))))\n",
        "        if hasattr(self, 'batch_2'):\n",
        "            x = self.batch_2(x)\n",
        "\n",
        "        x = self.pool_3(self.act_3(self.conv_3((x))))\n",
        "        if hasattr(self, 'batch_3'):\n",
        "            x = self.batch_3(x)\n",
        "\n",
        "        x = self.pool_4(self.act_4(self.conv_4((x))))\n",
        "        if hasattr(self, 'batch_4'):\n",
        "            x = self.batch_4(x)\n",
        "\n",
        "        x = self.pool_5(self.act_5(self.conv_5((x))))\n",
        "        if hasattr(self, 'batch_5'):\n",
        "            x = self.batch_5(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1_activation(self.fc1(x))\n",
        "        if hasattr(self, 'dropout1'):\n",
        "            x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the model\n",
        "model = SpeciesCNN(num_classes=10)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "wmOmzQAOz8f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_per_epoch(model, train_loader, loss_func, optimizer, epoch):\n",
        "    model.train(True)  # Set the model to training mode\n",
        "\n",
        "    train_loss = 0.0\n",
        "    correct_ans = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    for data in tqdm(train_loader):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        z, predicted = torch.max(outputs, 1)\n",
        "        correct_ans += (predicted == labels).sum().item()\n",
        "        num_samples += labels.size(0)\n",
        "\n",
        "\n",
        "    epoch_loss = train_loss / num_samples\n",
        "    epoch_accuracy = correct_ans / num_samples\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "emX4NI89z9Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_per_epoch(model, val_loader, loss_func, optimizer, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    correct_ans = 0\n",
        "    num_samples = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    for data in tqdm(val_loader):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            z, predicted = torch.max(outputs, 1)\n",
        "            correct_ans += (predicted == labels).sum().item()\n",
        "            num_samples += labels.size(0)\n",
        "\n",
        "    epoch_val_loss = val_loss / num_samples\n",
        "    epoch_val_accuracy = correct_ans / num_samples\n",
        "\n",
        "    return epoch_val_loss, epoch_val_accuracy\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFzi5NkiCk8r",
        "outputId": "9f130a93-3160-431a-ca97-3e5003874e7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SmallCNN(\n",
            "  (conv_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (act_1): ReLU()\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (act_2): ReLU()\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (act_3): ReLU()\n",
            "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (act_4): ReLU()\n",
            "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (act_5): ReLU()\n",
            "  (pool_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=25088, out_features=512, bias=True)\n",
            "  (fc1_activation): ReLU()\n",
            "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, loss_func, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_per_epoch(model, train_loader, loss_func, optimizer, epoch)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
        "        val_loss, val_acc = val_per_epoch(model, val_loader, loss_func, optimizer, epoch)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
        "        wandb.log({'train_loss': train_loss, 'train_accuracy': train_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc, 'epochs' : epoch + 1})"
      ],
      "metadata": {
        "id": "nTpFAHWq0DB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'val_accuracy',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'epochs' : {\n",
        "            'values' : [5]\n",
        "        },\n",
        "        'num_filters' : {\n",
        "            'values' : [32, 64, 128]\n",
        "        },\n",
        "        'filter_size' : {\n",
        "            'values' : [[3, 3, 3, 3, 3], [5, 5, 5, 5, 5]]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values' : [0.2, 0.3, 0.4, 0.5]\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values' : [16, 32, 64]\n",
        "        },\n",
        "        'data_aug' :  {\n",
        "            'values' : [True, False]\n",
        "        },\n",
        "        'batch_norm' : {\n",
        "            'values' : [True, False]\n",
        "        },\n",
        "        'learning_rate' : {\n",
        "            'values' : [1e-3, 5e-3, 1e-4, 2e-4]\n",
        "        },\n",
        "        'activation' : {\n",
        "            'values' : ['Mish', 'GELU', 'ReLU', 'LeakyReLU', 'SiLU']\n",
        "        },\n",
        "        'filter_org' : {\n",
        "            'values' : ['same', 'double', 'half']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "nZShbyrv0E8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assignment_2', entity = \"cs23m009\")"
      ],
      "metadata": {
        "id": "Ph-HIgzI0G8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        run_name = 'a_{}_bs_{}_lr_{}_e_{}_d_{}_aug_{}_bn_{}_fs_{}_fo_{}'.format(wandb.config.activation, wandb.config.batch_size, wandb.config.learning_rate, wandb.config.epochs, wandb.config.dropout, wandb.config.data_aug, wandb.config.batch_norm, wandb.config.filter_size, wandb.config.filter_org)\n",
        "        wandb.run.name = run_name\n",
        "\n",
        "        #Prepare Data\n",
        "        train_loader, val_loader, test_loader = prepare_dataset(wandb.config['batch_size'], wandb.config['data_aug'])\n",
        "        # Define the model\n",
        "        model = SpeciesCNN(num_classes=10, num_filters = wandb.config['num_filters'], filter_size = wandb.config['filter_size'], activation = wandb.config['activation'], batch_norm = wandb.config['batch_norm'], dropout_val=wandb.config['dropout'], filter_org = wandb.config['filter_org'])\n",
        "        model.to(device)\n",
        "        # Define loss function and optimizer\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=wandb.config['learning_rate'])\n",
        "        train_model(model, train_loader, val_loader, loss_func, optimizer, num_epochs=wandb.config['epochs'])\n",
        "\n",
        "wandb.agent(sweep_id, function = main, count = 10)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "oMkcg9Q-0HuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elM-psbMaW_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}